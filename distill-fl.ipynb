{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13431436,"sourceType":"datasetVersion","datasetId":8524982},{"sourceId":13431518,"sourceType":"datasetVersion","datasetId":8525040}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:17:46.429448Z","iopub.execute_input":"2025-10-19T09:17:46.429642Z","iopub.status.idle":"2025-10-19T09:18:00.890177Z","shell.execute_reply.started":"2025-10-19T09:17:46.429625Z","shell.execute_reply":"2025-10-19T09:18:00.889407Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (1.0.0rc2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nCollecting huggingface_hub\n  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface_hub, tokenizers, transformers\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface_hub-0.35.3 tokenizers-0.22.1 transformers-4.57.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from IPython.display import HTML, Javascript, display\n\ndef restart_kernel_and_run_all_cells():\n    display(HTML('''\n        <script>\n            // Restart the kernel\n            IPython.notebook.kernel.restart();\n            // Wait for a short period for the kernel to restart, then execute all cells\n            setTimeout(function(){\n                IPython.notebook.execute_all_cells();\n            }, 10000); // 10000 milliseconds = 10 seconds. Adjust as needed.\n        </script>\n    '''))\n\n# Call the function to restart the kernel and run all cells\nrestart_kernel_and_run_all_cells()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:18:00.891047Z","iopub.execute_input":"2025-10-19T09:18:00.891310Z","iopub.status.idle":"2025-10-19T09:18:00.898816Z","shell.execute_reply.started":"2025-10-19T09:18:00.891288Z","shell.execute_reply":"2025-10-19T09:18:00.898267Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n        <script>\n            // Restart the kernel\n            IPython.notebook.kernel.restart();\n            // Wait for a short period for the kernel to restart, then execute all cells\n            setTimeout(function(){\n                IPython.notebook.execute_all_cells();\n            }, 10000); // 10000 milliseconds = 10 seconds. Adjust as needed.\n        </script>\n    "},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:18:00.899462Z","iopub.execute_input":"2025-10-19T09:18:00.899639Z","iopub.status.idle":"2025-10-19T09:18:01.232725Z","shell.execute_reply.started":"2025-10-19T09:18:00.899625Z","shell.execute_reply":"2025-10-19T09:18:01.232111Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ancient/sample_submission.csv\n/kaggle/input/ancient/train.csv\n/kaggle/input/ancient/test.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nos.environ['WANDB_DISABLED'] = 'true'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:18:01.233465Z","iopub.execute_input":"2025-10-19T09:18:01.233740Z","iopub.status.idle":"2025-10-19T09:18:01.237166Z","shell.execute_reply.started":"2025-10-19T09:18:01.233722Z","shell.execute_reply":"2025-10-19T09:18:01.236588Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer\n)\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom torch.nn import CrossEntropyLoss\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:18:01.237990Z","iopub.execute_input":"2025-10-19T09:18:01.238349Z","iopub.status.idle":"2025-10-19T09:18:10.412942Z","shell.execute_reply.started":"2025-10-19T09:18:01.238326Z","shell.execute_reply":"2025-10-19T09:18:10.412328Z"}},"outputs":[{"name":"stderr","text":"2025-10-19 09:18:06.852027: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760865486.874473     123 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760865486.881213     123 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def load_data(csv_path, text_col='text', label_col='label'):\n    \"\"\"Load data from CSV\"\"\"\n    df = pd.read_csv(csv_path)\n    return df\n\ndef create_dataset(df, text_col='text', label_col='label', id_col='id'):\n    \"\"\"Convert pandas dataframe to HuggingFace dataset\"\"\"\n    data_dict = {\n        'text': df[text_col].tolist(),\n        'label': df[label_col].tolist()\n    }\n    if id_col and id_col in df.columns:\n        data_dict['id'] = df[id_col].tolist()\n    \n    dataset = Dataset.from_dict(data_dict)\n    return dataset\n\ndef tokenize_function(examples, tokenizer, max_length=512):\n    \"\"\"Tokenize texts\"\"\"\n    return tokenizer(\n        examples['text'],\n        padding='max_length',\n        truncation=True,\n        max_length=max_length,\n        return_tensors='pt'\n    )\n\ndef compute_metrics(eval_pred):\n    \"\"\"Compute metrics for evaluation\"\"\"\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    accuracy = accuracy_score(labels, predictions)\n    f1_macro = f1_score(labels, predictions, average='macro', zero_division=0)\n    precision = precision_score(labels, predictions, average='weighted', zero_division=0)\n    recall = recall_score(labels, predictions, average='weighted', zero_division=0)\n    \n    return {\n        'accuracy': accuracy,\n        'f1_macro': f1_macro,\n        'precision': precision,\n        'recall': recall\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:18:10.414542Z","iopub.execute_input":"2025-10-19T09:18:10.415124Z","iopub.status.idle":"2025-10-19T09:18:10.423093Z","shell.execute_reply.started":"2025-10-19T09:18:10.415101Z","shell.execute_reply":"2025-10-19T09:18:10.422269Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class FocalLoss(torch.nn.Module):\n    \"\"\"Focal Loss for handling class imbalance\"\"\"\n    \n    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n        \"\"\"\n        Args:\n            alpha: Weighting factor in range (0,1) to balance classes\n            gamma: Exponent of the modulating factor (1 - p_t) to focus on hard examples\n            reduction: 'mean', 'sum', or 'none'\n        \"\"\"\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n    \n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        p_t = torch.exp(-ce_loss)\n        focal_loss = (1 - p_t) ** self.gamma * ce_loss\n        \n        if self.alpha is not None:\n            if isinstance(self.alpha, (float, int)):\n                alpha_t = self.alpha\n            else:\n                alpha_t = self.alpha.gather(0, targets)\n            focal_loss = alpha_t * focal_loss\n        \n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\n# class FocalLossTrainer(Trainer):\n#     \"\"\"Custom Trainer with Focal Loss\"\"\"\n    \n#     def __init__(self, alpha=None, gamma=2.0, *args, **kwargs):\n#         super().__init__(*args, **kwargs)\n#         self.focal_loss = FocalLoss(alpha=alpha, gamma=gamma)\n    \n#     def compute_loss(self, model, inputs, return_outputs=False):\n#         labels = inputs.pop(\"labels\")\n#         outputs = model(**inputs)\n#         logits = outputs.logits\n        \n#         loss = self.focal_loss(logits, labels)\n        \n#         return (loss, outputs) if return_outputs else loss\nclass FocalLossTrainer(Trainer):\n    \"\"\"Custom Trainer with Focal Loss\"\"\"\n    \n    def __init__(self, alpha=None, gamma=2.0, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.focal_loss = FocalLoss(alpha=alpha, gamma=gamma)\n    \n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        \n        loss = self.focal_loss(logits, labels)\n        \n        return (loss, outputs) if return_outputs else loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:23:47.924645Z","iopub.execute_input":"2025-10-19T09:23:47.925227Z","iopub.status.idle":"2025-10-19T09:23:47.933785Z","shell.execute_reply.started":"2025-10-19T09:23:47.925199Z","shell.execute_reply":"2025-10-19T09:23:47.933206Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def fine_tune_model_with_focal_loss(\n    model_name,\n    train_dataset,\n    eval_dataset,\n    tokenizer,\n    num_labels,\n    output_dir,\n    num_epochs=3,\n    batch_size=16,\n    learning_rate=2e-5,\n    gamma=2.0,\n    class_weights=None\n):\n    \"\"\"Fine-tune model with Focal Loss (handles imbalanced data better)\"\"\"\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Fine-tuning {model_name} WITH FOCAL LOSS\")\n    print(f\"{'='*70}\\n\")\n    \n    # Tokenize datasets\n    train_dataset = train_dataset.map(\n        lambda x: tokenize_function(x, tokenizer),\n        batched=True,\n        remove_columns=['text', 'id'] if 'id' in train_dataset.column_names else ['text']\n    )\n    eval_dataset = eval_dataset.map(\n        lambda x: tokenize_function(x, tokenizer),\n        batched=True,\n        remove_columns=['text', 'id'] if 'id' in eval_dataset.column_names else ['text']\n    )\n    \n    print(\"✓ Tokenization complete\\n\")\n    torch.cuda.empty_cache()\n    \n    # Check GPU\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n        print(f\"✓ GPU Available: {torch.cuda.get_device_name(0)}\")\n        print(f\"✓ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\\n\")\n    else:\n        device = torch.device('cpu')\n        print(f\"✗ GPU NOT available, using CPU\\n\")\n    \n    # Load model\n    print(\"Loading model...\")\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=num_labels\n    )\n    \n    model.to(device)\n    model.config.pad_token_id = tokenizer.pad_token_id\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\\n\")\n    \n    # Prepare alpha weights for Focal Loss\n    alpha = None\n    if class_weights is not None:\n        alpha = torch.tensor(class_weights, device=device)\n        print(f\"Using class weights for Focal Loss\\n\")\n    \n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=f\"{output_dir}/{model_name.split('/')[-1]}_focal_loss\",\n        num_train_epochs=num_epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=100,\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        learning_rate=learning_rate,\n        save_total_limit=2,\n        load_best_model_at_end=True,\n        metric_for_best_model='f1_macro',\n        fp16=torch.cuda.is_available(),\n        gradient_accumulation_steps=2,\n        optim='adamw_torch',\n        report_to='none'\n    )\n    \n    # Trainer with Focal Loss\n    trainer = FocalLossTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=compute_metrics,\n        processing_class=tokenizer,\n        alpha=alpha,\n        gamma=gamma\n    )\n    \n    # Train\n    print(f\"Starting training with Focal Loss (gamma={gamma})...\\n\")\n    trainer.train()\n    \n    # Save model\n    model.save_pretrained(f\"{output_dir}/{model_name.split('/')[-1]}_focal_loss_final\")\n    \n    print(f\"\\n✓ Model saved to {output_dir}/{model_name.split('/')[-1]}_focal_loss_final\")\n    \n    return model, trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:23:51.191735Z","iopub.execute_input":"2025-10-19T09:23:51.192017Z","iopub.status.idle":"2025-10-19T09:23:51.202327Z","shell.execute_reply.started":"2025-10-19T09:23:51.191996Z","shell.execute_reply":"2025-10-19T09:23:51.201476Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Configuration\n    CSV_PATH = '/kaggle/input/ancient/train.csv'\n    TEXT_COL = 'text'\n    LABEL_COL = 'label'\n    ID_COL = 'id'\n    NUM_LABELS = 15\n    OUTPUT_DIR = '/kaggle/working/models'\n    MODEL_NAME = 'distilbert/distilbert-base-multilingual-cased'\n    \n    # Load data\n    print(\"Loading data...\")\n    df = load_data(CSV_PATH, TEXT_COL, LABEL_COL)\n    print(f\"Dataset shape: {df.shape}\")\n    print(f\"Label distribution:\\n{df[LABEL_COL].value_counts()}\\n\")\n    \n    # Load tokenizer\n    base_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    \n    # Convert to dataset\n    full_dataset = create_dataset(df, TEXT_COL, LABEL_COL, ID_COL)\n    \n    # Split into train and evaluation\n    split_dataset = full_dataset.train_test_split(test_size=0.2, seed=42)\n    train_dataset = split_dataset['train']\n    eval_dataset = split_dataset['test']\n    \n    print(f\"Train samples: {len(train_dataset)}\")\n    print(f\"Eval samples: {len(eval_dataset)}\\n\")\n    \n    # Calculate class weights for Focal Loss\n    label_counts = df[LABEL_COL].value_counts().sort_index()\n    total_samples = len(df)\n    class_weights = [total_samples / (NUM_LABELS * count) for count in label_counts]\n    print(f\"Class weights for Focal Loss: {class_weights}\\n\")\n    \n    try:\n        model_v2, trainer_v2 = fine_tune_model_with_focal_loss(\n            model_name=MODEL_NAME,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            tokenizer=base_tokenizer,\n            num_labels=NUM_LABELS,\n            output_dir=OUTPUT_DIR,\n            num_epochs=6,\n            batch_size=16,\n            learning_rate=2e-5,\n            gamma=2.0,\n            class_weights=class_weights\n        )\n        print(\"\\n\" + \"=\"*70)\n        print(\"✓ Version 2 (Focal Loss) Training Completed Successfully!\")\n        print(\"=\"*70)\n    except Exception as e:\n        print(f\"\\n✗ Error during Version 2 training: {str(e)}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:23:54.450954Z","iopub.execute_input":"2025-10-19T09:23:54.451578Z","iopub.status.idle":"2025-10-19T14:36:05.842956Z","shell.execute_reply.started":"2025-10-19T09:23:54.451552Z","shell.execute_reply":"2025-10-19T14:36:05.842039Z"}},"outputs":[{"name":"stdout","text":"Loading data...\nDataset shape: (119656, 3)\nLabel distribution:\nlabel\n7     35283\n8     18901\n5     16871\n0      9104\n6      7297\n3      5942\n1      5839\n10     5643\n4      5184\n2      4315\n11     2538\n9      2426\n12      169\n13      142\n14        2\nName: count, dtype: int64\n\nTrain samples: 95724\nEval samples: 23932\n\nClass weights for Focal Loss: [0.8762155828939661, 1.366170006279614, 1.8486828891463887, 1.3424884999439022, 1.5387860082304528, 0.4728271392725189, 1.0931981179480152, 0.22608810664248127, 0.4220446889935277, 3.288156086837043, 1.4136215960777365, 3.1430522721302863, 47.20157790927022, 56.17652582159624, 3988.5333333333333]\n\n\n======================================================================\nFine-tuning distilbert/distilbert-base-multilingual-cased WITH FOCAL LOSS\n======================================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/95724 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50ba3876a6b9454c967d52187b43ecd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/23932 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe9396e53d494fda8a4f2571d325e601"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"✓ Tokenization complete\n\n✓ GPU Available: Tesla T4\n✓ GPU Memory: 15.83 GB\n\nLoading model...\nTotal parameters: 135,336,207\nTrainable parameters: 135,336,207\n\nUsing class weights for Focal Loss\n\nStarting training with Focal Loss (gamma=2.0)...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8976' max='8976' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8976/8976 5:11:06, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1 Macro</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>5.249400</td>\n      <td>1.435677</td>\n      <td>0.440498</td>\n      <td>0.337746</td>\n      <td>0.515393</td>\n      <td>0.440498</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.421800</td>\n      <td>1.297715</td>\n      <td>0.503259</td>\n      <td>0.407854</td>\n      <td>0.589233</td>\n      <td>0.503259</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.188300</td>\n      <td>1.210757</td>\n      <td>0.531548</td>\n      <td>0.429966</td>\n      <td>0.606989</td>\n      <td>0.531548</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.902500</td>\n      <td>1.252524</td>\n      <td>0.543958</td>\n      <td>0.441927</td>\n      <td>0.612338</td>\n      <td>0.543958</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.786700</td>\n      <td>1.267177</td>\n      <td>0.559920</td>\n      <td>0.452744</td>\n      <td>0.614125</td>\n      <td>0.559920</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.796900</td>\n      <td>1.288280</td>\n      <td>0.561633</td>\n      <td>0.452041</td>\n      <td>0.617000</td>\n      <td>0.561633</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n✓ Model saved to /kaggle/working/models/distilbert-base-multilingual-cased_focal_loss_final\n\n======================================================================\n✓ Version 2 (Focal Loss) Training Completed Successfully!\n======================================================================\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom torch.utils.data import DataLoader, Dataset\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:37:56.449802Z","iopub.execute_input":"2025-10-19T14:37:56.450149Z","iopub.status.idle":"2025-10-19T14:37:56.454655Z","shell.execute_reply.started":"2025-10-19T14:37:56.450124Z","shell.execute_reply":"2025-10-19T14:37:56.454099Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class PredictionDataset(Dataset):\n    \"\"\"Dataset for making predictions\"\"\"\n    \n    def __init__(self, texts, tokenizer, max_length=512):\n        self.encodings = tokenizer(\n            texts,\n            padding='max_length',\n            truncation=True,\n            max_length=max_length,\n            return_tensors='pt'\n        )\n    \n    def __len__(self):\n        return len(self.encodings['input_ids'])\n    \n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.encodings['input_ids'][idx],\n            'attention_mask': self.encodings['attention_mask'][idx]\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:37:58.435294Z","iopub.execute_input":"2025-10-19T14:37:58.435578Z","iopub.status.idle":"2025-10-19T14:37:58.440868Z","shell.execute_reply.started":"2025-10-19T14:37:58.435559Z","shell.execute_reply":"2025-10-19T14:37:58.440217Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def make_predictions(\n    model,\n    tokenizer,\n    test_texts,\n    device,\n    batch_size=32\n):\n    \"\"\"Make predictions on test data\"\"\"\n    \n    print(\"\\nMaking predictions...\")\n    \n    dataset = PredictionDataset(test_texts, tokenizer)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    \n    all_predictions = []\n    all_probs = []\n    \n    model.eval()\n    with torch.no_grad():\n        for i, batch in enumerate(dataloader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            \n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=1)\n            probs = torch.softmax(logits, dim=1)\n            \n            all_predictions.extend(predictions.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n            \n            if (i + 1) % 10 == 0:\n                print(f\"  Processed {(i + 1) * batch_size} samples...\")\n    \n    print(f\"✓ Predictions completed for {len(all_predictions)} samples\\n\")\n    \n    return np.array(all_predictions), np.array(all_probs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:38:08.009356Z","iopub.execute_input":"2025-10-19T14:38:08.009637Z","iopub.status.idle":"2025-10-19T14:38:08.016483Z","shell.execute_reply.started":"2025-10-19T14:38:08.009615Z","shell.execute_reply":"2025-10-19T14:38:08.015663Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def predict_with_model(\n    test_csv_path,\n    model_dir,\n    model_name,\n    output_csv_path,\n    num_labels=15,\n    batch_size=32,\n    version_name=\"Model\"\n):\n    \"\"\"Make predictions using a trained model\"\"\"\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"PREDICTION: {version_name}\")\n    print(f\"{'='*70}\\n\")\n    \n    # Check GPU\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n        print(f\"✓ GPU Available: {torch.cuda.get_device_name(0)}\")\n        print(f\"✓ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\\n\")\n    else:\n        device = torch.device('cpu')\n        print(f\"✗ GPU NOT available, using CPU\\n\")\n    \n    # Load test data\n    print(f\"Loading test data from: {test_csv_path}\")\n    test_df = pd.read_csv(test_csv_path)\n    print(f\"Test dataset shape: {test_df.shape}\")\n    print(f\"Columns: {test_df.columns.tolist()}\\n\")\n    \n    # Load tokenizer\n    print(\"Loading tokenizer...\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    # Load model\n    print(f\"Loading model from: {model_dir}\")\n    if not os.path.exists(model_dir):\n        print(f\"✗ Model directory not found: {model_dir}\")\n        return None\n    \n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_dir,\n        num_labels=num_labels\n    )\n    model.to(device)\n    print(\"✓ Model loaded successfully\\n\")\n    \n    # Make predictions\n    predictions, probabilities = make_predictions(\n        model,\n        tokenizer,\n        test_df['text'].tolist(),\n        device,\n        batch_size=batch_size\n    )\n    \n    # Create output dataframe\n    output_df = pd.DataFrame({\n        'id': test_df['id'],\n        'label': predictions\n    })\n    \n    # Add confidence scores\n    output_df['confidence'] = np.max(probabilities, axis=1)\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n    \n    # Save predictions\n    output_df.to_csv(output_csv_path, index=False)\n    \n    print(f\"{'='*70}\")\n    print(f\"✓ Predictions saved to: {output_csv_path}\")\n    print(f\"{'='*70}\\n\")\n    \n    print(\"Sample predictions:\")\n    print(output_df.head(10))\n    \n    print(f\"\\nLabel distribution in predictions:\")\n    print(output_df['label'].value_counts().sort_index())\n    \n    print(f\"\\nPrediction statistics:\")\n    print(f\"  - Mean confidence: {output_df['confidence'].mean():.4f}\")\n    print(f\"  - Min confidence: {output_df['confidence'].min():.4f}\")\n    print(f\"  - Max confidence: {output_df['confidence'].max():.4f}\\n\")\n    \n    return output_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:38:11.336590Z","iopub.execute_input":"2025-10-19T14:38:11.336882Z","iopub.status.idle":"2025-10-19T14:38:11.346140Z","shell.execute_reply.started":"2025-10-19T14:38:11.336861Z","shell.execute_reply":"2025-10-19T14:38:11.345192Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    \n    # Configuration\n    TEST_CSV_PATH = '/kaggle/input/ancient/test.csv'\n    MODEL_NAME = 'distilbert/distilbert-base-multilingual-cased'\n    NUM_LABELS = 15\n    OUTPUT_DIR = '/kaggle/working/predictions'\n    \n    print(f\"{'='*70}\")\n    print(\"DISTILBERT PREDICTION SCRIPT\")\n    print(f\"{'='*70}\\n\")\n    \n    # ========================================================================\n    # VERSION 1: WITHOUT PEFT\n    # ========================================================================\n    \n    print(f\"\\n{'#'*70}\")\n    print(\"VERSION 1: WITHOUT PEFT\")\n    print(f\"{'#'*70}\")\n    \n    MODEL_DIR_NO_PEFT = '/kaggle/working/models/distilbert-base-multilingual-cased_focal_loss_final'\n    OUTPUT_CSV_NO_PEFT = f'{OUTPUT_DIR}/predictions_focal_loss.csv'\n    \n    try:\n        output_v1 = predict_with_model(\n            test_csv_path=TEST_CSV_PATH,\n            model_dir=MODEL_DIR_NO_PEFT,\n            model_name=MODEL_NAME,\n            output_csv_path=OUTPUT_CSV_NO_PEFT,\n            num_labels=NUM_LABELS,\n            batch_size=32,\n            version_name=\"No PEFT\"\n        )\n        print(\"✓ VERSION 1 COMPLETED SUCCESSFULLY!\\n\")\n    except Exception as e:\n        print(f\"✗ Error in VERSION 1: {str(e)}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T14:38:17.969930Z","iopub.execute_input":"2025-10-19T14:38:17.970688Z","iopub.status.idle":"2025-10-19T14:47:44.004562Z","shell.execute_reply.started":"2025-10-19T14:38:17.970663Z","shell.execute_reply":"2025-10-19T14:47:44.003714Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nDISTILBERT PREDICTION SCRIPT\n======================================================================\n\n\n######################################################################\nVERSION 1: WITHOUT PEFT\n######################################################################\n\n======================================================================\nPREDICTION: No PEFT\n======================================================================\n\n✓ GPU Available: Tesla T4\n✓ GPU Memory: 15.83 GB\n\nLoading test data from: /kaggle/input/ancient/test.csv\nTest dataset shape: (29914, 2)\nColumns: ['id', 'text']\n\nLoading tokenizer...\nLoading model from: /kaggle/working/models/distilbert-base-multilingual-cased_focal_loss_final\n✓ Model loaded successfully\n\n\nMaking predictions...\n  Processed 320 samples...\n  Processed 640 samples...\n  Processed 960 samples...\n  Processed 1280 samples...\n  Processed 1600 samples...\n  Processed 1920 samples...\n  Processed 2240 samples...\n  Processed 2560 samples...\n  Processed 2880 samples...\n  Processed 3200 samples...\n  Processed 3520 samples...\n  Processed 3840 samples...\n  Processed 4160 samples...\n  Processed 4480 samples...\n  Processed 4800 samples...\n  Processed 5120 samples...\n  Processed 5440 samples...\n  Processed 5760 samples...\n  Processed 6080 samples...\n  Processed 6400 samples...\n  Processed 6720 samples...\n  Processed 7040 samples...\n  Processed 7360 samples...\n  Processed 7680 samples...\n  Processed 8000 samples...\n  Processed 8320 samples...\n  Processed 8640 samples...\n  Processed 8960 samples...\n  Processed 9280 samples...\n  Processed 9600 samples...\n  Processed 9920 samples...\n  Processed 10240 samples...\n  Processed 10560 samples...\n  Processed 10880 samples...\n  Processed 11200 samples...\n  Processed 11520 samples...\n  Processed 11840 samples...\n  Processed 12160 samples...\n  Processed 12480 samples...\n  Processed 12800 samples...\n  Processed 13120 samples...\n  Processed 13440 samples...\n  Processed 13760 samples...\n  Processed 14080 samples...\n  Processed 14400 samples...\n  Processed 14720 samples...\n  Processed 15040 samples...\n  Processed 15360 samples...\n  Processed 15680 samples...\n  Processed 16000 samples...\n  Processed 16320 samples...\n  Processed 16640 samples...\n  Processed 16960 samples...\n  Processed 17280 samples...\n  Processed 17600 samples...\n  Processed 17920 samples...\n  Processed 18240 samples...\n  Processed 18560 samples...\n  Processed 18880 samples...\n  Processed 19200 samples...\n  Processed 19520 samples...\n  Processed 19840 samples...\n  Processed 20160 samples...\n  Processed 20480 samples...\n  Processed 20800 samples...\n  Processed 21120 samples...\n  Processed 21440 samples...\n  Processed 21760 samples...\n  Processed 22080 samples...\n  Processed 22400 samples...\n  Processed 22720 samples...\n  Processed 23040 samples...\n  Processed 23360 samples...\n  Processed 23680 samples...\n  Processed 24000 samples...\n  Processed 24320 samples...\n  Processed 24640 samples...\n  Processed 24960 samples...\n  Processed 25280 samples...\n  Processed 25600 samples...\n  Processed 25920 samples...\n  Processed 26240 samples...\n  Processed 26560 samples...\n  Processed 26880 samples...\n  Processed 27200 samples...\n  Processed 27520 samples...\n  Processed 27840 samples...\n  Processed 28160 samples...\n  Processed 28480 samples...\n  Processed 28800 samples...\n  Processed 29120 samples...\n  Processed 29440 samples...\n  Processed 29760 samples...\n✓ Predictions completed for 29914 samples\n\n======================================================================\n✓ Predictions saved to: /kaggle/working/predictions/predictions_focal_loss.csv\n======================================================================\n\nSample predictions:\n       id  label  confidence\n0  114610      0    0.513406\n1    1146      8    0.356753\n2   39252      2    0.360343\n3   56935      7    0.496307\n4  113433      6    0.322606\n5  129106      2    0.436179\n6  138583     10    0.450514\n7   53192      8    0.839966\n8   94399      4    0.312094\n9  148092      5    0.431406\n\nLabel distribution in predictions:\nlabel\n0     2134\n1     2098\n2     1412\n3     1963\n4     1782\n5     3343\n6     2046\n7     5608\n8     4992\n9     1363\n10    1789\n11    1204\n12      69\n13     111\nName: count, dtype: int64\n\nPrediction statistics:\n  - Mean confidence: 0.5340\n  - Min confidence: 0.1304\n  - Max confidence: 0.9563\n\n✓ VERSION 1 COMPLETED SUCCESSFULLY!\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}